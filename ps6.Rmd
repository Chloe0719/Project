---
title: "ps6_ECON_491_AML_qiuer2"
author: "Qiuer Cai"
date: "2023-03-30"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##Question 1

a)(iii) As we increase lambda from 0, the training RSS will steadily increase. Because the model is less flexible, and it will increases the training RSS.

b)(ii) The test RSS will decreases initially, and then eventually start increasing in a U shape. Because the model is lees flexible, the betaj coefficient is being restricted. So it will decrease first, then it changes the directions and increases in a U shape.

c)(iv) The varaince will steadily decrease because of the model with less flexibility.

d)(iii) The bias will streadily increase, because the model is less flexible as lambda increases, so the bias allso increase when beta j coefficients are being restricted.

e)(v) The irreducible error will remain constant, because there is no relationship between it and the model parameters.

#Question 2
```{r}
#a) splite the data into training and testing set
library(ISLR2)
data(College)
set.seed(100)

sample=sample(c(TRUE, FALSE), nrow(College), replace=TRUE, prob=c(0.7,0.3))
train=College[sample, ]
test=College[!sample, ]
```

```{r}
#b) fit a linear model using least squares
lm.fit=lm(Apps~.,data=train)
lm.pred=predict(lm.fit,test)
lm.MSE=mean((lm.pred-test$Apps)^2)
cat('The linear regression model MSE is:',lm.MSE)
```
```{r}
#c) fit a ridge regression model on the taining set
library(glmnet)
set.seed(100)
train.matrix=model.matrix(Apps~.,data=train)
test.matrix=model.matrix(Apps~.,data=test)

grid=10^seq(10,-2,length=100)
ridge.mod <- glmnet(train.matrix, train$Apps, alpha=0,lambda=grid, thresh=1e-12)

cv.ridge=cv.glmnet(train.matrix, train$Apps, alpha=0,lambda=grid, thresh=1e-12)

#selecting the best lambda
plot(cv.ridge)
best.lambda=cv.ridge$lambda.min
cat('The best lambda is:', best.lambda)
```
```{r}
ridge.pred <- predict(ridge.mod, s=best.lambda, newx=test.matrix)
ridge.MSE <- mean((ridge.pred-test$Apps)^2)
cat('The Ridge Regression model MSE is: ', ridge.MSE)
```


```{r}
#d) fit a lasso model on the training set
lasso.mod=glmnet(train.matrix, train$Apps, alpha=1,lambda=grid, thresh=1e-12)

cv.lasso=cv.glmnet(train.matrix, train$Apps, alpha=1,lambda=grid, thresh=1e-12)

plot(cv.lasso)
best.lambda.lasso=cv.lasso$lambda.min
cat('The best lambda is:', best.lambda.lasso)
```

```{r}
lasso.pred <- predict(lasso.mod, s=best.lambda.lasso, newx=test.matrix)
lasso.MSE <- mean((lasso.pred-test$Apps)^2)
cat('The lasso model MSE is: ', lasso.MSE)
```
```{r}
lasso.coef=predict(lasso.mod, s=best.lambda.lasso, type ="coefficients",)[1:19,]
lasso.coef[lasso.coef!=0]
```

##Question 3
```{r}
#a)
set.seed(1)
X=rnorm(100)
e=rnorm(100)
```

```{r}
#b) 
beta0=4
beta1=1
beta2=-4
beta3=0.2
Y = beta0+beta1*X+beta2*(X^2)+beta3*(X^3)+e
```

```{r}
#c)
library(leaps)
library(glmnet)
data.lasso=data.frame(y=Y,x=X,x2=X^2,x3=X^3,x4=X^4,x5=X^5,x6=X^6,x7=X^7,x8=X^8,x9=X^9,x10=X^10)

x_m=model.matrix(y~.,data.lasso)[,-1]
y_m=data.lasso$y

grid.lasso=10^seq(10,-2,length=100)
train.lasso=sample(1:nrow(x_m),nrow(x_m)/2)
test.lasso=y_m[-train.lasso]


data.lasso.mod=glmnet(x_m[train.lasso,],y_m[train.lasso], alpha=1, lambda=grid.lasso)
cv.lasso.out=cv.glmnet(x_m[train.lasso,],y_m[train.lasso], alpha=1)
plot(cv.lasso.out)
```

```{r}
best.lambda1=cv.lasso.out$lambda.min
lasso.pred2=predict(data.lasso.mod, s=best.lambda1,newx=x_m[-train.lasso,])
#MSE
MSE=mean((lasso.pred2-test.lasso)^2)
cat('The MSE for this lasso model is',MSE)

```

```{r}
data.mod=glmnet(x_m,y_m,alpha=1,lambda=grid.lasso)
lasso.coef2=predict(data.mod, s=best.lambda1, type='coefficients')[1:11,]
lasso.coef2[lasso.coef2!=0]
```

For this question, the beta0, beta2 is actually predict well with the model. Because the coefficient number is close to what I set up for the model. 


```{r}
#d) Find the best subset selection
beta7=6
Y2=beta0+beta7*(X^7)+e

data2=data.frame(y2=Y2,x=X,x=X^7)
best.subset=regsubsets(y2~., data=data2,nvmax=3)
best.subset.summary=summary(best.subset)

```


```{r}
#Find Cp,BIC, adjusted R^2
min.Cp=which.min(best.subset.summary$cp)
min.BIC=which.min(best.subset.summary$bic)
max.adjusted_R=which.max(best.subset.summary$adjr)

```

```{r}
plot(best.subset.summary$cp, xlab = "Subset", ylab = "Cp", type = "l",pch=20)
points(min.Cp, best.subset.summary$cp[min.Cp], col = "red", lwd=6,pch = 20)

plot(best.subset.summary$bic, xlab = "Subset", ylab = "BIC", type = "l",pch=20)
points(min.BIC, best.subset.summary$bic[min.BIC], col = "red", lwd=6,pch = 20)

plot(best.subset.summary$adjr, xlab = "Subset", ylab = "Adjusted R", type = "l",pch=20)
points(max.adjusted_R, best.subset.summary$adjr[max.adjusted_R],col = "red", lwd=6,pch = 20)
```

```{r}
coef(best.subset,id=1)
```


```{r}
#lasso
x_m_2=model.matrix(y2~.,data=data2)[,-1]
y_m_2=data2$y2
train_2=sample(1:nrow(x_m_2),nrow(x_m_2)/2)
test_2=y_m_2[-train_2]


lasso.mod2=glmnet(x_m_2[train_2, ], y_m_2[train_2], alpha = 1,lambda=grid.lasso)

cv.out_2=cv.glmnet(x_m_2[train_2,], y_m_2[train_2], alpha=1)
plot(cv.out_2)


```

```{r}
best.lambda_2=cv.out_2$lambda.min
lasso.pred_2=predict(lasso.mod2,s=best.lambda_2,newx=x_m_2[-train_2,])

MSE_2=mean((lasso.pred_2-test_2)^2)
cat('The MSE for this lasso model is:', MSE_2)
```
```{r}
lasso.mod_2=glmnet(x_m_2,y_m_2,alpha=1,lambda=grid.lasso)
lasso.coef_2=predict(lasso.mod_2, s=best.lambda_2, type='coefficients')[1:3,]
lasso.coef_2[lasso.coef_2!=0]
```

Conclusion:Based on question(4), there is only 1 variable is the best for subset selection with BIC. And there is only 1 varaible is best for subset selection with both Cp and adjusted R. For the lasso, the coefficients for question(d) is 5.75, which is close to the number that we set for beta7. So the lasso model is a better model. But the intercept I set is 4, and the predicted intercept is a little over that number. However, the best subset model predict the intercept and beta7 better than lasso.

